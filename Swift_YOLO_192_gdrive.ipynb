{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcory-hub/swift-yolo/blob/main/Swift_YOLO_192_gdrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Swift-YOLO 192×192 model with a dataset from Google Drive\n",
        "This notebook trains a **Swift-YOLO** (tiny, 192×192) object-detection model using [ModelAssistant](https://github.com/Seeed-Studio/ModelAssistant) (SSCMA). Your dataset lives in Google Drive; the notebook copies it into Colab and runs training."
      ],
      "metadata": {
        "id": "jmFr-Sd-MFNi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEegpjhP2xWJ"
      },
      "source": [
        "**Category:** Object Detection\n",
        "\n",
        "**Algorithm:** [Swift-YOLO](configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py)\n",
        "\n",
        "**Dataset:** Custom dataset from Google Drive (requires Gmail account)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup instructions\n",
        "\n",
        "Do these steps **before** running the notebook.\n",
        "\n",
        "### Step 1: Enable GPU\n",
        "\n",
        "1. In Colab: **Runtime** → **Change runtime type**\n",
        "2. Set **Hardware accelerator** to **GPU** (choose **A100** if available for faster training)\n",
        "3. Click **Save**\n",
        "\n",
        "**Expected environment** (check the first code cell output):\n",
        "- GPU: T4 or higher (A100 preferred for long runs)\n",
        "- Python 3.11.x\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Prepare your dataset in COCO format\n",
        "\n",
        "1. **Folder structure** — Use this layout (exact names). Each of `train/` and `valid/` must contain images and one `_annotations.coco.json`:\n",
        "\n",
        "```\n",
        "dataset\n",
        "├── train\n",
        "│   ├── _annotations.coco.json\n",
        "│   ├── image1.jpg\n",
        "│   ├── image2.jpg\n",
        "│   └── ...\n",
        "└── valid\n",
        "    ├── _annotations.coco.json\n",
        "    ├── image3.jpg\n",
        "    ├── image4.jpg\n",
        "    └── ...\n",
        "```\n",
        "\n",
        "2. **Zip the dataset** — Create `dataset.zip` from the `dataset` folder. On macOS/Linux you can exclude metadata with:\n",
        "\n",
        "   `zip -r dataset.zip dataset -i '*.jpg' '*.jpeg' '*.png' '*.json'`\n",
        "\n",
        "3. **Upload to Google Drive** — Put `dataset.zip` in the root directory. Optionally put a custom pretrained checkpoint there too (e.g. `best_coco_bbox_mAP_epoch_100.pth`) if you want to fine-tune from your own model.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Set number of classes\n",
        "\n",
        "⚠️ **Update the class count to match your dataset.**\n",
        "\n",
        "- **Default in this notebook:** 4 classes\n",
        "- **What to do:** In the training cell, set `num_classes` (and in the config override) to your actual number of classes (e.g. `num_classes=5` for 5 classes).\n",
        "\n",
        "---\n",
        "\n",
        "## Ready to run\n",
        "\n",
        "Run the cells in order. The notebook will:\n",
        "\n",
        "1. Mount Google Drive and check GPU/Python\n",
        "2. Clone ModelAssistant and install dependencies\n",
        "3. Download pretrained weights and copy your dataset from Drive\n",
        "4. Train Swift-YOLO (default: 3 epochs for a quick test; use 100+ for real training)\n",
        "5. Zip the output folder so you can download checkpoints and exports"
      ],
      "metadata": {
        "id": "lfEu9VRl5GTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Check GPU and Python version"
      ],
      "metadata": {
        "id": "fFIYDnTB14IT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V6Hespbd2xWL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "!nvidia-smi\n",
        "\n",
        "# Check Python version\n",
        "!python --version"
      ],
      "metadata": {
        "id": "gKht7yXrNzKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlf8_bNg2xWK"
      },
      "source": [
        "## 2. Prerequisites: clone ModelAssistant and install SSCMA\n",
        "Clone the [ModelAssistant](https://github.com/Seeed-Studio/ModelAssistant) repo and run the Colab setup script. You can ignore any non-fatal errors from the setup script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v97_kWgU2xWK"
      },
      "outputs": [],
      "source": [
        "!pip install ethos-u-vela\n",
        "!git clone https://github.com/Seeed-Studio/ModelAssistant.git -b 2.0.0  #clone the repo\n",
        "%cd ModelAssistant\n",
        "!. ./scripts/setup_colab.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mKfjj3T2xWL"
      },
      "source": [
        "\n",
        "## 3. Download pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SewV9Zvp2xWL"
      },
      "outputs": [],
      "source": [
        "%mkdir -p Swift-YOLO_192\n",
        "!wget -c https://files.seeedstudio.com/sscma/model_zoo/detection/animal/animal_detection.pth -O Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT5ctFj-2xWL"
      },
      "source": [
        "## 4. Copy dataset from Google Drive\n",
        "\n",
        "Copies `dataset.zip` from `/content/drive/MyDrive/yolo/` into Colab and unzips it under `Swift-YOLO_192/`. Ensure you created the `yolo` folder and put `dataset.zip` there (see Setup Step 2). Overwrites if files already excist.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/dataset.zip' '/content/dataset.zip'\n",
        "!unzip -o '/content/dataset.zip' -d '/content/ModelAssistant/Swift-YOLO_192/'"
      ],
      "metadata": {
        "id": "Vuhw1EfJFaHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: use your own pretrained checkpoint\n",
        "If you have a trained Swift-YOLO checkpoint in Drive (e.g. `best_coco_bbox_mAP_epoch_100.pth`), copy it to `Swift-YOLO_192/pretrain.pth` to fine-tune from it. Put the file in `/content/drive/MyDrive/` and adjust the filename below if needed."
      ],
      "metadata": {
        "id": "XMXpPH2j1YEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/best_coco_bbox_mAP_epoch_100.pth' 'Swift-YOLO_192/pretrain.pth'"
      ],
      "metadata": {
        "id": "zJUqD0EzIKgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5XfzBgq2xWL"
      },
      "source": [
        "## 5. Train Swift-YOLO with SSCMA\n",
        "\n",
        "Training uses the config `swift_yolo_tiny_1xb16_300e_coco.py`. Key parameters (overridden with `--cfg-options`):\n",
        "\n",
        "| Parameter | Description |\n",
        "|-----------|-------------|\n",
        "| `data_root` | Path to dataset (train/valid with COCO JSONs) |\n",
        "| `epochs` | Number of training epochs (**3 here = quick test**; use 100+ for real training) |\n",
        "| `batch` | Batch size |\n",
        "| `height`, `width` | Input size (192×192 for Swift-YOLO) |\n",
        "| `load_from` | Pretrained checkpoint path |\n",
        "| `num_classes` | Number of classes in your dataset |\n",
        "\n",
        "Change these in the cell below to match your dataset and goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qPELuZQ2xWL"
      },
      "outputs": [],
      "source": [
        "!sscma.train configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py \\\n",
        "--cfg-options  \\\n",
        "    work_dir=Swift-YOLO_192 \\\n",
        "    num_classes=4 \\\n",
        "    epochs=3  \\\n",
        "    height=192 \\\n",
        "    width=192 \\\n",
        "    batch=256 \\\n",
        "    lr=0.16 \\\n",
        "    data_root=Swift-YOLO_192/dataset/ \\\n",
        "    load_from=Swift-YOLO_192/pretrain.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Zip and download the Swift-YOLO_192 folder\n",
        "\n",
        "Packs checkpoints and exported files from `Swift-YOLO_192/` into a zip and triggers download. You can then export to `int8_vela.tflite` for Grove Vision AI V2 (see Deploy below) or use the `.pth` checkpoint for further training."
      ],
      "metadata": {
        "id": "L-umWJwvW_bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Define the paths\n",
        "folder_path = '/content/ModelAssistant/Swift-YOLO_192'\n",
        "zip_filename = 'Swift-YOLO_Models.zip'\n",
        "\n",
        "# Create the zip file\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # List all items in the directory\n",
        "    for item in os.listdir(folder_path):\n",
        "        item_path = os.path.join(folder_path, item)\n",
        "\n",
        "        # Only add to zip if it's a file (skips all subdirectories)\n",
        "        if os.path.isfile(item_path):\n",
        "            zipf.write(item_path, arcname=item)\n",
        "            print(f\"Added to zip: {item}\")\n",
        "\n",
        "# Automatically trigger the download to your computer\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "v4SvKLwcWw0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}